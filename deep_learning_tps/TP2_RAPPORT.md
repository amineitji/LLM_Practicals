# TP2 - GPT-2 Implementation & Pre-training

## Vue d'ensemble

Impl√©mentation compl√®te d'un mod√®le GPT-2 (124M param√®tres) from scratch et entra√Ænement sur corpus texte.

---

## 1. Layer Normalization

**Concept :** Normaliser les activations pour stabiliser l'entra√Ænement.

**Formule :** `(x - mean) / sqrt(variance + eps)`

![LayerNorm](outputs/tp2_layernorm.png)

**R√©sultat :**
- Avant : valeurs dispers√©es (0 √† 0.5)
- Apr√®s : valeurs centr√©es autour de 0
- Mean ‚âà 0, Variance ‚âà 1

**Pourquoi ?** R√©duit le covariate shift, permet learning rates plus √©lev√©s.

---

## 2. GELU Activation

**Formule :** GELU(x) = 0.5 √ó x √ó (1 + tanh[...])

![GELU](outputs/tp2_gelu.png)

**GELU vs ReLU :**
- **ReLU** : Coupure dure √† 0
- **GELU** : Transition douce (probabiliste)
- **Avantage** : Pas de "dying neurons", gradient m√™me pour valeurs n√©gatives

**FeedForward :** Linear(768‚Üí3072) ‚Üí GELU ‚Üí Linear(3072‚Üí768)

---

## 3. Residual Connections

**Concept :** `output = layer(x) + x` (skip connection)

![Residual](outputs/tp2_residual.png)

**Impact sur les gradients :**

**Sans residual :**
```
Layer 1: 0.00020  ‚Üê Tr√®s faible
Layer 2: 0.00012
Layer 5: 0.00505
```

**Avec residual :**
```
Layer 1: 0.222    ‚Üê Beaucoup plus fort !
Layer 2: 0.207
Layer 5: 1.326
```

**Pourquoi ?** R√©sout le vanishing gradient, permet r√©seaux profonds (12+ couches).

---

## 4. Transformer Block

**Architecture :**
```
Input ‚Üí LayerNorm ‚Üí Attention ‚Üí Dropout ‚Üí +Residual
      ‚Üí LayerNorm ‚Üí FeedForward ‚Üí Dropout ‚Üí +Residual ‚Üí Output
```

**Param√®tres par bloc :** 7,085,568
- Attention : 2,360,064
- FeedForward : 4,722,432
- LayerNorms : 3,072

---

## 5. GPT-2 Complet

**Pipeline :**
```
Token IDs [batch, seq]
    ‚Üì
Token + Position Embeddings
    ‚Üì
12√ó Transformer Blocks
    ‚Üì
LayerNorm final
    ‚Üì
Linear (768 ‚Üí 50257)
    ‚Üì
Logits [batch, seq, vocab_size]
```

**Param√®tres totaux :** 163,009,536
- Uniques : 124,412,160 (~124M)
- Taille : 621 MB (float32)

---

## 6-7. G√©n√©ration de texte

**M√©thode greedy :**
1. Calculer logits
2. Softmax ‚Üí probabilit√©s
3. Argmax ‚Üí token le plus probable
4. Ajouter au contexte
5. R√©p√©ter

**Avant entra√Ænement :**
```
Input:  "Hello, I am"
Output: "Hello, I am Featureiman Byeswickattribute argue"
```
‚Üí Incoh√©rent (poids al√©atoires)

---

## 8. Loss & Perplexity

**Test sur mod√®le non-entra√Æn√© :**
- Pr√©dictions : "Armed he Netflix" vs "effort moves you"
- Cross Entropy : 10.794
- **Perplexity : 48,726** (tr√®s √©lev√©e = tr√®s confus)

---

## 9. Entra√Ænement

**Donn√©es :**
- Corpus : the-verdict.txt (20K caract√®res)
- Train : 18K (90%) / Val : 2K (10%)
- 10 epochs, AdamW optimizer

**√âvolution :**

| Epoch | Train Loss | Val Loss | Exemple g√©n√©ration |
|-------|-----------|----------|-------------------|
| 1 | 9.83 | 9.98 | "Every effort moves you,,,,,,,,,,,," |
| 2 | 6.81 | 7.06 | "Every effort moves you, and, and, and..." |
| 5 | 4.59 | 6.25 | "Every effort moves you, and, and he was..." |
| 10 | 1.12 | 6.28 | **"Yes--quite insensible to the irony. She wanted him vindicated--and by me!"** |

![Training](outputs/tp2_training.png)

**R√©sultat :**
- ‚úÖ Train loss : 10.98 ‚Üí 1.12 (-90%)
- ‚úÖ G√©n√©ration coh√©rente et r√©aliste
- ‚ö†Ô∏è Val loss stagne √† 6.28 (overfitting l√©ger)

---

## 10-11. Temperature & Top-K Sampling

**Temperature :** Contr√¥le la diversit√©

![Temperature](outputs/tp2_temperature.png)

- **Temp 0.1** : Tr√®s conservatif (toujours m√™me mot)
- **Temp 1.0** : Normal
- **Temp 5.0** : Tr√®s cr√©atif (mais peut perdre coh√©rence)

**Top-K :** Garde seulement les k meilleurs tokens

![Top-K](outputs/tp2_topk.png)

- Sans top-k : Tous les tokens possibles
- Avec top-k=3 : Seulement "forward", "toward", "closer"

**Formule :** `logits / temperature` puis softmax + sampling

---

## 12. G√©n√©ration avanc√©e

**Nouvelle fonction :**
```python
def generate(model, idx, max_new_tokens, temperature=1.0, top_k=25):
    # Temperature scaling + top-k filtering
    # Multinomial sampling au lieu d'argmax
```

**R√©sultat :**
- Non-d√©terministe (2 g√©n√©rations diff√©rentes)
- Plus de diversit√©
- Contr√¥le cr√©ativit√© vs coh√©rence

**Exemples (temp=1.4, top-k=25) :**
```
Output 1: "Every effort moves you?"
          "Yes--quite insensible to the irony..."

Output 2: "Every effort moves you?"
          "Yes--quite insensible to the portrait..."
```

---

## 13. Sauvegarde/Chargement

**Fichiers cr√©√©s :**

1. **gpt2_trained.pth** (620 MB)
   - State dict du mod√®le entra√Æn√©

2. **gpt2_checkpoint.pth** (1.86 GB)
   - Mod√®le + optimizer + m√©tadonn√©es
   - Permet de reprendre l'entra√Ænement

**Chargement :**
```python
model.load_state_dict(torch.load('model.pth'), strict=False)
```
‚Üí `strict=False` pour ignorer les buffers (mask)

---

## R√©sultats finaux

### ‚úÖ Accomplissements

1. **Impl√©ment√© GPT-2 from scratch** (124M param√®tres)
2. **Entra√Æn√© le mod√®le** (loss 10.98 ‚Üí 1.12)
3. **G√©n√©ration coh√©rente** apr√®s 10 epochs
4. **Sampling avanc√©** (temperature + top-k)
5. **Sauvegarde/chargement** fonctionnel

### üìä M√©triques finales

- Train loss : **1.116**
- Val loss : **6.281**
- Perplexity val : **~535** (vs 48,726 au d√©part)
- Taille mod√®le : **621 MB**

### üéØ Exemple de g√©n√©ration (epoch 10)

**Prompt :** "Every effort moves you"

**Output :**
```
"Every effort moves you?"

"Yes--quite insensible to the irony. She wanted him 
vindicated--and by me!"

"Oh, and back his head to look up at the sketch of 
the donkey."
```

‚Üí **Dialogue r√©aliste avec structure narrative coh√©rente !**

---

## Visualisations

Toutes les visualisations sont dans `/outputs/` :
- `tp2_layernorm.png` - Normalisation des activations
- `tp2_gelu.png` - Comparaison activations
- `tp2_residual.png` - Impact des skip connections
- `tp2_training.png` - √âvolution des losses
- `tp2_temperature.png` - Impact de la temp√©rature
- `tp2_topk.png` - Filtrage top-k

---

## Concepts cl√©s ma√Ætris√©s

‚úÖ **LayerNorm** - Stabilisation de l'entra√Ænement  
‚úÖ **GELU** - Activation moderne pour transformers  
‚úÖ **Residual connections** - R√©seaux profonds  
‚úÖ **Transformer blocks** - Architecture modulaire  
‚úÖ **Pre-training** - Entra√Ænement de LLM  
‚úÖ **Sampling** - G√©n√©ration contr√¥l√©e  
‚úÖ **Model persistence** - Sauvegarde/chargement  

---

## Conclusion

**Mission accomplie !** üéâ

Tu as construit et entra√Æn√© un vrai mod√®le GPT-2 from scratch. Le mod√®le g√©n√®re du texte coh√©rent apr√®s seulement 10 epochs sur un petit corpus. 

**Tu comprends maintenant comment ChatGPT fonctionne √† l'int√©rieur.**